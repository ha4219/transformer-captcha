{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, paths, processor, max_target_length=128):\n",
    "        self.paths = paths\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        fn = self.paths[idx]\n",
    "        text = fn.split('/')[-1].split('.')[0]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(fn).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(\n",
    "            text, \n",
    "            padding=\"max_length\", \n",
    "            max_length=self.max_target_length\n",
    "        ).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TrOCRProcessor\n",
    "import glob\n",
    "from conf import *\n",
    "\n",
    "all_path = glob.glob(path)\n",
    "train_path, test_path = train_test_split(all_path)\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-small-printed\")\n",
    "train_dataset = IAMDataset(paths=train_path, processor=processor)\n",
    "eval_dataset = IAMDataset(paths=test_path, processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 963\n",
      "Number of validation examples: 321\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([3, 384, 384])\n",
      "labels torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "encoding = train_dataset[0]\n",
    "for k,v in encoding.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=200x50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(train_path[0]).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yew6p\n"
     ]
    }
   ],
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.21k/4.21k [00:00<00:00, 1.73MB/s]\n",
      "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 246M/246M [00:11<00:00, 20.8MB/s] \n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-printed and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading (â€¦)neration_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190/190 [00:00<00:00, 77.9kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): DeiTModel(\n",
       "    (embeddings): DeiTEmbeddings(\n",
       "      (patch_embeddings): DeiTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): DeiTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): DeiTPooler(\n",
       "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): Embedding(64044, 256, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n",
       "        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=256, out_features=64044, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-small-printed\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3343/152175726.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  cer_metric = load_metric(\"cer\")\n",
      "Downloading builder script: 5.59kB [00:00, 1.81MB/s]                   \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "cer_metric = load_metric(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cer(pred_ids, label_ids):\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongha/anaconda3/envs/tc/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 0.9013779094961824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongha/anaconda3/envs/tc/lib/python3.10/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (64) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:06<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.19232270349700661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 0.6423505231982372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.1947037539425292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 2: 0.5786205579022892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.14346506950944862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 3: 0.5790677285585247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.13616091906846672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 4: 0.4604645488692112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.12965154803348125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 5: 0.42905174415619646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:06<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.13257685321430712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 6: 0.4266230937887411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:09<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.21580220198487574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 7: 0.42699190632241674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:06<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.178208660513755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 8: 0.3915820918122276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:07<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.19309508400803768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 9: 0.3447146938472498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.12530126820745038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10: 0.2916770688578731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.09219085635942467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 11: 0.1373440002198102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.07506239653569294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 12: 0.0943853527872411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.09960431908071031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 13: 0.294118335256811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.10576588458039134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 14: 0.06721227985547214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.06692533458382738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 15: 0.031020801789203627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.06984370323126872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 16: 0.02192464774497403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.05896450432114769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:16<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 17: 0.022821260276479554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.06477294042235734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 18: 0.0359686182308026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.06927667714636092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 19: 0.1192722674459219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.06815047954088192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 20: 0.018118035284893922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.06977123829459468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 21: 0.012271319111412177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.0601586766878963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:15<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 22: 0.010805079950111323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.05902303961601293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:16<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 23: 0.004688884547652036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.0557347689167834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:16<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 24: 0.002764809685835584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.05059762994001498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:16<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 25: 0.002704108120552951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.052251769291230384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:16<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 26: 0.025381528816308032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.09165276489187575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:16<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 27: 0.07688909336222244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:05<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.08068604717365838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/61 [00:08<00:07,  3.69it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m     18\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 19\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/tc/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tc/lib/python3.10/site-packages/transformers/optimization.py:446\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[39m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[1;32m    445\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m(\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta1))\n\u001b[0;32m--> 446\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39;49maddcmul_(grad, grad, value\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[1;32m    447\u001b[0m denom \u001b[39m=\u001b[39m exp_avg_sq\u001b[39m.\u001b[39msqrt()\u001b[39m.\u001b[39madd_(group[\u001b[39m\"\u001b[39m\u001b[39meps\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    449\u001b[0m step_size \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(300):  # loop over the dataset multiple times\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # get the inputs\n",
    "        for k,v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n",
    "        \n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    valid_cer = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_dataloader):\n",
    "            # run batch generation\n",
    "            outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
    "            # compute metrics\n",
    "            cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
    "            valid_cer += cer \n",
    "    \n",
    "    total_cer = valid_cer / len(eval_dataloader)\n",
    "    print(\"Validation CER:\", total_cer)\n",
    "    if total_cer < 0.01:\n",
    "        import datetime\n",
    "        save_pretrained_dir = f'save/{total_cer}_{epoch}_{datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9),\"JST\")).strftime(\"%Y%m%dT%H%M%S\")}'\n",
    "        model.save_pretrained(save_pretrained_dir)\n",
    "\n",
    "model.save_pretrained(\".\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
